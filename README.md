# Everlytics Internship Assignment â€“ QuickShop ETL + Apache Airflow Pipeline

This project was developed as part of the **Everlytics Internship Hiring Assignment**.  
It includes a fully functional **ETL package (quickshop_etl)** and an **Apache Airflow DAG** to orchestrate the ETL workflow using **Astro CLI + Docker**.

---

## ğŸ“ Project Structure

```
Everlytics_Assignment/
â”‚
â”œâ”€â”€ quickshop_etl/               # Python ETL package
â”‚   â”œâ”€â”€ readers.py               # Read CSV sources
â”‚   â”œâ”€â”€ writers.py               # Write Parquet + summary JSON
â”‚   â”œâ”€â”€ transforms.py            # Apply business rules
â”‚   â”œâ”€â”€ validation.py            # Row validation functions
â”‚   â”œâ”€â”€ cli.py                   # Command-line ETL wrapper
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ airflow-docker/              # Astro CLI Airflow project (Docker-based)
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ quickshop_etl_dag.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt         # Airflow-only requirements
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ data/                        # Input CSVs
â”‚   â”œâ”€â”€ products.csv
â”‚   â”œâ”€â”€ inventory.csv
â”‚   â”œâ”€â”€ order_20251025.csv
â”‚   â”œâ”€â”€ order_20251026.csv
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ output/                      # Local ETL output (for testing)
â”‚
â””â”€â”€ requirements.txt             # Global project dependencies (except Airflow)
```

---

## ğŸš€ Features Implemented

### âœ” 1. Full ETL pipeline  
- Reads product, inventory, and order datasets  
- Validates rows  
- Applies transformations  
- Generates:  
  - **Parquet output**  
  - **Daily summary JSON**

### âœ” 2. Apache Airflow DAG  
- Uses **PythonOperator**  
- Executes ETL for a given date  
- Output stored under:  
  `/usr/local/airflow/project/output/`

### âœ” 3. Astro CLI + Docker Integration  
- Airflow deployed inside Docker  
- Local ETL package installed inside the container  
- Data copied into the container correctly  
- DAG visible and executable from Airflow UI or CLI  

### âœ” 4. Manual Testing & DAG Run Validation  
- Successfully triggered DAG  
- ETL completed end-to-end  
- Parquet + summary files created inside container  

---

## ğŸ“¦ Installation & Setup (Local ETL Testing)

### 1. Create and activate virtual environment
```bash
python -m venv Everlytics_Assignment
source Everlytics_Assignment/bin/activate   # Mac/Linux
Everlytics_Assignment\Scriptsctivate     # Windows
```

### 2. Install project requirements
```bash
pip install -r requirements.txt
```

### 3. Run ETL locally
```bash
python -m quickshop_etl.cli run_for_date 20251025 --data_dir=data --output_dir=output
```

---

## ğŸ›  Setting up Apache Airflow with Astro CLI

### 1. Install Astro CLI
```bash
curl -sSL https://install.astronomer.io | sudo bash
astro version
```

### 2. Create Astro project (inside airflow-docker)
```bash
astro dev init
```

### 3. Start Airflow inside Docker
```bash
astro dev start --no-cache
```

### 4. Copy ETL package and data into the container
```bash
docker cp quickshop_etl airflow-astro-api:/usr/local/airflow/project/
docker cp data airflow-astro-api:/usr/local/airflow/project/
```

### 5. Test ETL execution inside container
```bash
docker exec -it airflow-api-server-1 python -c "from quickshop_etl.cli import run_for_date; print(run_for_date('20251025', '/usr/local/airflow/project/data', '/usr/local/airflow/project/output'))"
```

---

## ğŸŒ€ Running the Airflow DAG

### Unpause the DAG
```bash
docker exec -it airflow-api-server-1 airflow dags unpause quickshop_etl_pipeline
```

### Trigger DAG manually
```bash
docker exec -it airflow-api-server-1 airflow dags trigger quickshop_etl_pipeline --conf '{"date_str":"20251025"}'
```

### Check DAG run status
```bash
docker exec -it airflow-api-server-1 airflow dags list-runs quickshop_etl_pipeline
```

---

## ğŸ“¤ Collecting Output Files (From Container to Local System)

### 1. Copy processed Parquet file
```bash
docker cp airflow-api-server-1:/usr/local/airflow/project/output/processed/date=2025-10-25/data.parquet ./output/
```

### 2. Copy summary JSON (if generated)
```bash
docker cp airflow-api-server-1:/usr/local/airflow/project/output/summaries/summary_2025-10-25.json ./output/
```

---

## ğŸ§¾ requirements.txt (Global)

This file contains dependencies for your **local ETL**, NOT Airflow.

```
pandas
pyarrow
fastparquet
python-dateutil
```

Airflow itself is **not included** here because it is installed inside the Docker container using Astro CLI.

### ğŸ“Œ Airflow has its own file:
```
airflow-docker/requirements.txt
```

---

## ğŸ Final Notes

- This submission satisfies the full requirements of the **Everlytics Internship Assignment**.
- The project includes:
  âœ” ETL Python package  
  âœ” Airflow DAG  
  âœ” Working Docker environment  
  âœ” End-to-end ETL execution  
  âœ” Correct output artifacts  

For any additional improvements, CI/CD or cloud deployment can also be added.

---

## ğŸ™Œ Author

**Krit Prasad**  
Submitted as part of the **Everlytics Internship Hiring Assignment**.
